集成学习
每个学习器的加权结合
集成学习是否比简单学习的效果好：错误概率小于0.5时集成学习好，否则集成学习好
每个简单学习一模一样集成学习回和单个学习一模一样
每个base learner需要相互独立，很难保证，毕竟训练集相同
监督学习有N个样本
要保证模型之间负相关
拉姆他等于0，会退化为base learner,朗姆他等于1，单个训练等于全部训练。
连续属性与离散属性
一，NCL
	enmsanble learning的性能会随着basic learning增加而增加
    每个basic learning都有两部分，一个是损失函数，一个是basic之间的相关。
二，bagging:（bootstrap aggregating）
	不同的学习器在不同的数据上训练
    有放回的选取
    对于stable的不适合用bagging来做
    提高数据集的稳定性
    边界上的点更加重要

三，boosting & Adaboost:
	有一个迭代的方法处理未分类的数据：处理学习器处理不好的部分
    权重越大越容易被抽中
    第一个关键：如何迭代这个weight
    第二个问题：如何集成
    通过这两个问题可以生成不同的boosting算法
    为了提升不同basic之间的多样性
    第二个是要提高单个学习期的性能
四，随机森林：（可以用于回归问题）
    	增强不同的basic learn之间的多样性
        每个basiclearn尽可能的

集成学习多用于监督学习

总结：
	原始数据：
    分成不同的datasets，选取n个不同的采样。（不同的采样为了增加diversity）
    	不同的datasets要尽可能的不同。
    构建basiclearner，比较直接的。（不同的basic也是为了增加diversity）
    最后combine在一起，不同的basiclearner会有加权，对于权重又进行了一次学习。