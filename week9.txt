集成学习
每个学习器的加权结合
集成学习是否比简单学习的效果好：错误概率小于0.5时集成学习好，否则集成学习好
每个简单学习一模一样集成学习回和单个学习一模一样
每个base learner需要相互独立，很难保证，毕竟训练集相同
监督学习有N个样本
要保证模型之间负相关
拉姆他等于0，会退化为base learner,朗姆他等于1，单个训练等于全部训练。
连续属性与离散属性
NCL
bagging:（bootstrap aggregating）
	不同的学习器在不同的数据上训练
    有放回的选取
    对于stable的不适合用bagging来做

boosting & Adaboost:
	有一个迭代的方法处理未分类的数据：处理学习器处理不好的部分
    权重越大越容易被抽中
    第一个关键：如何迭代这个weight
    第二个问题：如何集成
    通过这两个问题可以生成不同的boosting算法
    为了提升不同basic之间的多样性
    第二个是要提高单个学习期的性能